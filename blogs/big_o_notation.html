<!DOCTYPE html>
<html>
<head>
  <title>Jennifer Fox</title>
  <link rel="stylesheet" type="text/css" href="../style/blogpostings.css">   
  <link href="https://fonts.googleapis.com/css?family=Montserrat|Raleway" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<body>
  <header>
    <nav>
      <a href="./index.html">Home</a>
      <a href="./index.html#who">About Me</a>
      <a href="./index.html#experience">Experience</a>
      <a href="./index.html#projects">Projects</a>
      <a href="./index.html#contact">Contact</a>
      <a href="../blog.html">Blog</a>
    </nav>

    <div id="main">
      <h1>What is Big 0 Notation?</h1>
      <div class="share"><p><i> Share this article.</i></p>
        <a href="#" class="fa fa-twitter"></a>
        <a href="#" class="fa fa-linkedin"></a>
        <a href="#" class="fa fa-github"></a>
      </div>
    
  </div>
  </header>

  <div class="content"> 
  <h2>What the heck is Big O?</h2>
      <p>When you look up Big O Notation, it’s defined as a way for computer scientist and mathematicians to express the limiting behaviour of a function or the time complexity of an algorithm. If you are like me, that definition is simply overwhelming. Let’s break down that jargon and understand what the heck is a Big O Notation.</p>
      <p>Just imagine you are in a book store where none of the books have been sorted at all. Your task is to sort all the books. You would have to brainstorm the best way to sort the books that will take you the least amount of time and will not require you to remember every single book in the store. In this analogy, you are the computer and the book are the data. When we want to run any kind of process or set of rules on a data set, we have to think about the best way to do this both with respect to the time it will take the amount of space or memory it will consume. So how do we determine which methods or algorithms will consume the least memory and the least time? Mathematicians and computer scientist have created Big O Notation for this.<br>
     <br>Big O Notation is used to describe the performance or complexity of particular algorithms. When looking up an algorithm, you will typically find it’s Best Case O Notation and it’s Worst Case O Notation. As you can assume, best case is the O function that it can do if everything is set up right. But, as you’ve probably heard, most computer scientist are lazy and data is messy. So, Worst Case Big O Notation has become convention. We want to know how much time and memory theoretically will this algorithm take in it’s worst possible situation.<br>
     <br>Big O Notations are simply functions of both resources used and data size. Although I will go over some common orders of growth, the best way to understand this is the graph visual below. The Big O Notation that uses the least amount of resources for even the biggest of the data sets is obviously the best. Mathematicians and computer scientist work every day to try and create algorithms that reach this goal. In our growing age of tech, it has become essential that algorithms can handle big data without much resources. <br></p>      
      
      <h2>A pretty graph</h2>
      <p>This will make sense in a minute once you read, but I find it helpful to look at as you read along.</p>
      <iframe src="https://www.desmos.com/calculator/zseevnrzip?embed" width="500px" height="500px" style="border: 1px solid #ccc" frameborder=0></iframe>
      <p>The y axis represents the resources used by the computer (time and memory), the x axis represents the data size. </p>

      <h2>Common Orders of Growth</h2>
      <h3>O(1)</h3>
      <p>Essentially this function reads y = 1 (in the graph it is the color red). This is the absolute best kind of algorithm. Essentially this means that no matter what your data size, the algorithm will execute in the same amount of time and memory. If you are familiar with a little bit of code, an example of this would be accessing an array index. If that doesn’t make sense to you, an example of this is going into a library and using the indexing system to find the reading book you were looking for. Doesn’t matter how many books the library has, you are simply accessing one thing in an ordered list. No matter if you go to a small library or the library of congress, it will take you just as much time and memory to find that book. </p>
      
      <h3>O(N)</h3>
      <p>Essentially this function reads y=x (in the graph it is the color blue). Whatever the data size is, it is proportional to how much time and space it will take to execute. The resources used by the algorithm grows linearly with the data size. If you are familiar with code, an example of this is to run a task through each item in a list. Like printing each item in a list on a new line. If you have 2 items to print, it will print twice. If you have 30 items to print, it will print 30 times. A good example of this is looking for your favorite CD (old school, I know) in a stack of CD’s. If you have 5 CD’s, it won’t take you very long. If you have 50 CD’s, you’ll need to go through each one individually and it might take it a while. </p>
      
      <h3>O(N<sup>2</sup>)</h3>
      <p>Essentially this function reads y= x<sup>2</sup> (in the graph it is the color green). This algorithms performance is directly proportional to the square of the size of the data set. Basically this means that as your data gets bigger, your resources are consumed at an exponential rate. If you are a coder, an example of this would be nested for loops. So imagine you have a list of people and for each of these people you have their age, city, and job title. If you want to print each person’s information, you would iterate through the list of names and for each name iterate through a new list of the individual’s info. If you only have 2 people to iterate through, it might not take long. But 200 people, now we are talking about a large amount of memory and time consumed by the computer to do this task. For programmers, it’s really essential to understand this concept. Each nested loop you add to your code, your resources will be consumed by one more power. For example, 3 nested loops will consume N<sup>3</sup> resources, and 4 nested loops will consume N<sup>4</sup> resources. Keeping this in mind when writing code is essentially to understanding how to make your code clean and efficient.<br><br> For non-programmers out there, imagine that you have you have to find duplicates in a deck of cards. You would sort through the cards once to find each number, then sort through again finding all the cards that are duplicates. The more cards you have, it will be exponentially more time and physically space (you’ll need a huge table to sort through 100 cards versus 10) to sort all the cards. </p>
      
      <h3>O(2<sup>N</sup>)</h3>
      <p>Essentially this function reads y=2<sup>x</sup> (this graph is in the color orange). The resources used for these algorithms grows double for each additional data set. This is pretty fast growth and not ideal for an algorithm. If you are a coder, think recurions. A recursion must run a function within a function. Therefore, with each new data point, two functions are ran. It’s really helpful to understand that if you are using recursions to solve your problem, you can expect that code to be using a lot of time and memory to run. If you are a non programmer, see my blog posting on recursions. Not only are recursions mind-blogging, they are wicked helpful to understand. </p>
      
      <h3>O(logN)</h3>
      <p>Essentially this function reads y=log(x) (this graph is the color purple). The resources used for these algorithms will start pretty slow and then eventually increase over time. These kind of algorithms are best for large data sets as the size of the data set has minimum effect on the resources used for a while. Logarithms are a bit harder to grasp and understand, but the big idea is that the bigger the data set, there is some increase in the resources used, but not that much. The best analogy: looking someone up in a phone book. You are looking for a particular name alphabetically. So you open it up to approximately the right spot and then begin flipping the pages. The bigger the phone book, the longer it will take for you to look up that name, but it won’t make a huge impact.</p>

</div> 
 <footer>
<p>blog by Jen Fox</p>
  </footer>
</body>
</html>